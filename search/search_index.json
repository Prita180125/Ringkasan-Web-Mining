{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Web Mining A 2021 Nama : Prita Dyah Anindhita NPM : 180411100125 Dosen Pengampu Mata Kuliah : Mula'ab, S.Si., M.Kom. Crawling Teori tentang Crawling Kita semua tahu bahwa dalam website terkandung informasi atau data. Dalam web mining, crawling adalah suatu proses ekstraksi data dari suatu website. Artinya, kita dapat menghimpun data atau informasi tersebut baik secara manual maupun otomatis. Biasanya, crawling digunakan untuk mengambil data dalam jumlah yang banyak dan tidak mungkin untuk disalin satu persatu secara manual. Crawling data dapat dilakukan dengan berbagai cara dan bahasa pemrograman yang beragam pula. Dalam penjelasan kali ini, saya menggunakan bahasa pemrograman Python dengan library Scrapy untuk menghimpun data dari suatu web e-commerce. Lihat disini untuk dokumentasi Scrapy Python. Implementasi Crawling Contoh crawling yang akan saya lakukan adalah mengambil data produk dari League World yang merupakan website penjualan pakaian dan perlengkapan olahraga. Pertama, tentukan terlebih dahulu page/halaman mana yang akan diambil datanya (dalam hal ini, saya mengambil data produk women collection). Mulai project melalui command prompt C:\\Users> scrapy startproject tugas_scrape Masuk ke dalam direktori project yang telah dibuat C:\\Users> cd tugas_scrape Memasukkan link website yang dituju C:\\Users\\tugas_scrape> scrapy genspider league https://www.league-world.com/ Setelah itu, buka folder project yang telah dibuat. Akan terdapat folder dengan nama spiders berisi file init dan file dengan nama league. Edit pada teks editor python. # -*- coding: utf-8 -*- import scrapy class LeagueSpider(scrapy.Spider): name = 'league' allowed_domains = ['https://www.league-world.com/'] start_urls = ['https://www.league-world.com/tag/29/women/'] def parse(self, response): data = response.css('.product-item') for item in data: nama = item.css('.product-name a::text').extract() harga = item.css('.autoNumeric::text').extract() yield{ 'Nama Barang' : nama, 'Harga Barang' : harga } Dalam kode diatas, saya mengekstrak nama produk dan harga dari halaman produk wanita. Untuk mengambil data nama barang dan harga, saya menggunakan bantuan css seperti yang terdapat pada kode program. Untuk mengekstrak data dari web page tersebut, gunakan perintah pada command prompt sebagai berikut: C:\\Users\\tugas_scrape> scrapy crawl league Untuk menyimpan data hasil crawling ke dalam bentuk file lain (saya simpan dalam bentuk file berekstensi csv yang sudah disiapkan sebelumnya), dapat menggunakan perintah berikut: C:\\Users\\tugas_scrape> scrapy crawl league -o hasil.csv Text Preprocessing Text preprocessing dapat kita artikan sebagai proses untuk menyiapkan teks sebelum dimodelkan lebih lanjut sesuai kebutuhan. Text preprocessing terdiri dari beberapa hal seperti case folding, tokenizing, stemming, stopword removal, dll. Text preprocessing biasanya juga digunakan dalam peringkasan dokumen. Tahapan-tahapan seperti case folding, tokenizing, stemming, stopword removing, dan pembentukan matriks kata sangat membantu dalam meringkas dokumen. Case Folding Teori tentang Case Folding Case folding merupakan bagian dari text preprocessing untuk 'merapikan' teks yang akan dimodelkan. Teks selalu ditulis dalam aturan yang umum seperti penggunaan huruf kapital, adanya penggunaan angka ataupun tanda baca dan simbol-simbol. Peran case folding disini adalah mengubah seluruh huruf dalam teks menjadi huruf kecil, menghilangkan angka, dan menghapus tanda baca sehingga teks menjadi lebih rapi untuk diproses ke tahap selanjutnya. Implementasi Case Folding Mengubah seluruh huruf dalam teks menjadi huruf kecil Untuk mengubah huruf menjadi lowercase seluruhnya, dapat menggunakan perintah bawaan dari python yaitu .lower() teks = \"Prita adalah mahasiswa tahun ke-3 Program Studi S1 Teknik Informatika di Universitas Trunojoyo Madura. Karena masih situasi pandemi, Prita menempuh perkuliahan dari rumah secara daring.\" teks_baru = teks.lower() print(teks_baru) Output akan seperti berikut ini: prita adalah mahasiswa tahun ke-3 program studi s1 teknik informatika di universitas trunojoyo madura. karena masih situasi pandemi, prita menempuh perkuliahan dari rumah secara daring. Menghilangkan angka dalam teks Untuk menghilangkan angka yang ada di dalam teks, dapat menggunakan perintah bawaan dari python sebagai berikut: import re teks = \"Prita adalah mahasiswa tahun ke-3 Program Studi S1 Teknik Informatika di Universitas Trunojoyo Madura. Karena masih situasi pandemi, Prita menempuh perkuliahan dari rumah secara daring.\" teks_baru = re.sub(r\"\\d+\", \"\", teks) print(teks_baru) Output akan seperti berikut ini: Prita adalah mahasiswa tahun ke- Program Studi S Teknik Informatika di Universitas Trunojoyo Madura. Karena masih situasi pandemi, Prita menempuh perkuliahan dari rumah secara daring. Menghilangkan tanda baca dalam teks Untuk menghapus tanda baca yang ada di dalam teks, dapat menggunakan kode sebagai berikut: import string teks = \"Prita adalah mahasiswa tahun ke-3 Program Studi S1 Teknik Informatika di Universitas Trunojoyo Madura. Karena masih situasi pandemi, Prita menempuh perkuliahan dari rumah secara daring.\" teks_baru = teks.translate(str.maketrans(\"\",\"\",string.punctuation)) print(teks_baru) Output akan seperti berikut ini: Prita adalah mahasiswa tahun ke3 Program Studi S1 Teknik Informatika di Universitas Trunojoyo Madura Karena masih situasi pandemi Prita menempuh perkuliahan dari rumah secara daring Tokenisasi Teori tentang Tokenisasi Tokenizing atau tokenisasi dapat diartikan sebagai proses pemisahan kata-kata dalam suatu teks. Kata demi kata tersebut dipisahkan dan dikenal sebagai token. Dalam Python sendiri, ada syntax yang dapat digunakan untuk memisahkan kata dalam suatu kalimat/paragraf yaitu perintah split(). Namun biasanya dalam suatu paragraf terdapat kata yang berulang kali digunakan. Penggunaan perintah split() tidak lagi dapat membantu karena perintah tersebut hanya memisahkan kata tanpa mengenali apakah kata tersebut sudah ada sebelumnya atau belum. Untuk itu, ada modul NLTK milik Python yang memang ditujukan untuk Text Preprocesing. Perintah yang digunakan adalah word_tokenize() Implementasi Tokenisasi import nltk from nltk.tokenize import word_tokenize from nltk.probability import FreqDist import string teks = \"Prita adalah mahasiswa tahun ke-3 Program Studi Teknik Informatika di Universitas Trunojoyo Madura. Sebelum pandemi terjadi, biasanya Prita berangkat dari rumah ke kos menaiki bus. Setelah pandemi ada di Indonesia, Prita sama sekali belum kembali ke kos.\" teks = teks.translate(str.maketrans('','',string.punctuation)) kata = nltk.tokenize.word_tokenize(teks) frekuensi = nltk.FreqDist(kata) print(frekuensi.most_common()) Output akan seperti berikut ini: [('Prita', 3), ('di', 2), ('pandemi', 2), ('ke', 2), ('kos', 2), ('adalah', 1), ('mahasiswa', 1), ('tahun', 1), ('ke3', 1), ('Program', 1), ('Studi', 1), ('Teknik', 1), ('Informatika', 1), ('Universitas', 1), ('Trunojoyo', 1), ('Madura', 1), ('Sebelum', 1), ('terjadi', 1), ('biasanya', 1), ('berangkat', 1), ('dari', 1), ('rumah', 1), ('menaiki', 1), ('bus', 1), ('Setelah', 1), ('ada', 1), ('Indonesia', 1), ('sama', 1), ('sekali', 1), ('belum', 1), ('kembali', 1)] Stemming Teori tentang Stemming Stemming adalah suatu proses untuk mengidentifikasi kata dasar dari suatu kata yang ada. Misalnya, ada kata \"melihat\" maka ketika dilakukan proses stemming, akan diperoleh kata dasar yaitu \"lihat\". Dalam Python, untuk melakukan stemming dapat menggunakan library NLTK, namun ada library khusus yang diciptakan untuk stemming kata dalam bahasa Indonesia yaitu Python Sastrawi. Implementasi Stemming from Sastrawi.Stemmer.StemmerFactory import StemmerFactory factory = StemmerFactory() stemmer = factory.create_stemmer() teks = \"Prita selalu melakukan olahraga setiap hari selama satu jam. Menurut Prita, olahraga mampu membantunya memperbaiki pola tidur di malam hari.\" teks_baru = stemmer.stem(teks) print(teks_baru) Outputnya akan seperti ini: prita selalu laku olahraga tiap hari lama satu jam turut prita olahraga mampu bantu baik pola tidur di malam hari Stopword Removal Teori tentang Stopword Dalam sebuah teks yang panjang, seringkali terdapat kata yang tidak terlalu penting seperti kata hubung \"di\", \"ke\", \"yang\" dan lain sebagainya. Kata-kata tersebut sebaiknya tidak terlalu diperhatikan karena memang tidak penting seperti kata lain yang mungkin saja menjadi kunci dari suatu teks. Kata-kata yang tidak penting tersebut dikenal sebagai stopword. Implementasi Stopword Removal from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from nltk.tokenize import word_tokenize factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover() teks = \"Dilaporkan pada Selasa, 8 Juni 2021 terjadi kecelakaan tunggal di Jalan Pattimura. Korban adalah siswa SMA bernama Doni yang baru pulang sekolah. Korban mengaku mengantuk sehingga menabrak pohon di tepi jalan. Kondisi korban saat ini baik-baik saja dan hanya mengalami luka ringan.\" teks_baru = stopword.remove(teks) print(teks_baru) Output akan seperti ini: Dilaporkan Selasa, 8 Juni 2021 terjadi kecelakaan tunggal Jalan Pattimura. Korban siswa SMA bernama Doni baru pulang sekolah. Korban mengaku mengantuk menabrak pohon tepi jalan. Kondisi korban ini baik-baik dan mengalami luka ringan. Reduksi Dimensi Teori Dalam pengolahan dokumen yang berbasis teks, tentu saja terdapat banyak kosakata. Ketika beragam kosakata tersebut berjumlah sangat banyak, maka diperlukan reduksi dimensi berupa seleksi fitur. Fitur perlu diseleksi untuk menghindari kemunculan berulang suatu fitur dan hanya memilih fitur-fitur yang penting saja. Reduksi dimensi dengan melakukan seleksi fitur dapat diterapkan dengan menggunakan matriks TF-IDF (Term Frequency-Inverse Document Frequency). Dalam matriks TF-IDF, kolom dan barisnya didasarkan pada kosakata dan jumlah kalimat dalam dokumen. Isinya sendiri merupakan frekuensi kemunculan kata dalam setiap potong kalimat. Setelah terbentuk matriks, baru dilakukan reduksi dimensi. Implementasi Untuk membentuk matriks TF-IDF, dapat menggunakan library sklearn dari Python dan mengimport modul TfidfTransformer. Lalu untuk reduksi dimensi menggunakan PCA. Dokumen harus melalui case folding, stemming, tokenizing, dan stopword removal untuk sampai ke tahap ini. Berikut adalah tampilan dari matriks TFIDF Untuk mereduksi dimensi, gunakan code berikut: from sklearn.decomposition import PCA pca = PCA (n_components=2) fit_pca = pca.fit_transform(dfb) pca_df = pd.DataFrame(data=fit_pca, columns=['PCA 1', 'PCA 2']) pca_df.tail() Output yang dihasilkan: PCA 1 PCA 2 4 -0.019008 -0.278487 5 -0.291685 -0.128199 6 -0.052512 0.532792 7 0.251088 0.735093 8 0.726751 -0.208282 Implementasi Text Preprocessing import numpy as np import PyPDF2 import doctest import sys from IPython.display import Image from requests_html import HTMLSession import matplotlib.pyplot as plt %matplotlib inline import networkx as nx from nltk.tokenize.punkt import PunktSentenceTokenizer from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer session = HTMLSession() r = session.get('https://suarabojonegoro.com/news/2021/03/15/kapolres-bojonegoro-beri-penghargaan-kepada-kades-kasun-dan-ketua-rukun-tetangga-telah-bantu-percepatan-tangani-covid-19') articles = r.html.find('div.entry-content') for item in articles: newsitem = item.find('div.entry-content-single', first=True) news = newsitem.text print(news) doc_tokenizer = PunktSentenceTokenizer() sentences_list = doc_tokenizer.tokenize(news) type(sentences_list) import string from Sastrawi.Stemmer.StemmerFactory import StemmerFactory# create stemmer factory = StemmerFactory() stemmer = factory.create_stemmer() import re dokumenre=[] for i in sentences_list: hasil = re.sub(r\"\\d+\", \"\", i) dokumenre.append(hasil) print(dokumenre) len(dokumenre) dokumen=[] for i in dokumenre: hasil = i.replace('\\n','') dokumen.append(hasil) print(dokumen) from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from nltk.tokenize import word_tokenize factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover() #a=cv_matrix.toarray() a=len(dokumen) dokumenstop=[] for i in range(0, a): sentence = stopword.remove(dokumen[i]) dokumenstop.append(sentence) print(dokumenstop) cv = CountVectorizer() cv_matrix = cv.fit_transform(dokumen) a=cv_matrix.toarray() a.shape from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from nltk.tokenize import word_tokenize factory = StopWordRemoverFactory() dokumenstop=[] for i in dokumen: output = i.translate(str.maketrans(\"\",\"\",string.punctuation)) dokumenstop.append(output) print(dokumenstop) factory = StemmerFactory() stemmer = factory.create_stemmer() dokumenstem=[] for i in dokumenstop: output = stemmer.stem(i) dokumenstem.append(output) print(dokumenstem) bag = cv.fit_transform(dokumenstem) print(cv.vocabulary_) print(cv.get_feature_names()) len(cv.get_feature_names()) bag = cv.fit_transform(dokumenstem) matrik_vsm=bag.toarray() matrik_vsm.shape matrik_vsm[0] import pandas as pd a=cv.get_feature_names() print(len(matrik_vsm[:,1])) print(len(matrik_vsm[:,1])) dfb =pd.DataFrame(data=matrik_vsm,index=list(range(1, len(matrik_vsm[:,1])+1, )),columns=[a]) dfb from sklearn.feature_extraction.text import TfidfTransformer tfidf = TfidfTransformer(use_idf=True,norm='l2',smooth_idf=True) tf=tfidf.fit_transform(cv.fit_transform(dokumenstem)).toarray() dfb =pd.DataFrame(data=tf,index=list(range(1, len(tf[:,1])+1, )),columns=[a]) dfb Modelling Teori Text classification merupakan salah satu hal dalam pengolahan bahasa alami. Biasanya, text classification digunakan dalam analisis sentimen (bisa diterapkan untuk analisis sentimen media sosial), pelabelan topik, dan lain-lain. Implementasi Implementasi text classification melibatkan text preprocessing, maka saya akan memberikan contoh code untuk meringkas dokumen menggunakan metode LSA (Latent Semantic Analysis) pada teks yang telah melalui preprocessing. doc_complete = ['roni pergi rumah nenek desa', 'sana banyak pohon hijau sejuk teduh'] doc_complete from sklearn.feature_extraction.text import TfidfVectorizer vectorizer = TfidfVectorizer() X =vectorizer.fit_transform(doc_complete) print(X) from sklearn.decomposition import TruncatedSVD lsa = TruncatedSVD(n_components=2,n_iter=100) lsa.fit(X) terms = vectorizer.get_feature_names() for i,comp in enumerate(lsa.components_): termsInComp = zip(terms,comp) sortedterms = sorted(termsInComp, key=lambda x: x[1],reverse=True)[:10] print(\"Concept %d:\" % i) for term in sortedterms: print(term[0]) print(\" \") Output program: Concept 0: desa nenek pergi roni rumah hijau pohon sana sejuk teduh Concept 1: hijau pohon sana sejuk teduh banyak desa nenek pergi ron Evaluasi Teori Evaluasi ringkasan otomatis dapat dilakukan dengan banyak metode. Yang paling sering digunakan adalah ROUGE-n. ROUGE sendiri merupakan seperangkat metrik untuk mengevaluasi ringkasan teks otomatis yang bekerja dengan cara membandingkan suatu ringkasan otomatis atau terjemahan dengan seperangkat rangkuman referensi. Pengukuran ROUGE terbagi menjadi beberapa: ROUGE-N yang digunakan untuk mengukur unigram, bigram, trigram, dan n-gram yang lebih tinggi lagi ROUGE-L yang digunakan untuk mengukur pencocokan kata terpanjang menggunakan LCS. ROUGE-S yang juga bisa disebut sebagai skip-gram cooccurence. Misalnya, skip-bigram mengukur overlap dari pasangan kata yang memiliki jumlah gaps maksimum sebanyak dua pada setiap kata. ROUGE-SU merupakan pengembangan dari ROUGE-S yang memperhatikan unigram. ROUGE-W yang menggunakan LCS namun tidak memberikan preferensi pada kalimat yang emiliki kata-kata yang lebih berurutan. Untuk mengukur keakuratan ringkasan harus menghitung Precision, Recall, dan F-Measure. Rumus menghitung recall adalah banyaknya kata yang overlap dibagi dengan banyaknya kata pada ringkasan rujukan. Rumus menghitung Precision adalah banyaknya kata yang overlap dibagi dengan banyaknya kata pada ringkasan mesin Rumus menghitung F-Measure adalah Implementasi from rouge_score import rouge_scorer hypotesis = ['Godric Gryffindor is the bravest wizard', 'Salazar Slytherin is the smartest wizard'] reference = ['Gryffindor house is for the bravest one', 'Slytherin house is for the smartest one'] scorer = rouge_scorer.RougeScorer(['rouge1']) results = {'precision': [], 'recall': [], 'fmeasure': []} for (h, r) in zip(hypotesis, reference): score = scorer.score(h, r) precision, recall, fmeasure = score['rouge1'] results['precision'].append(precision) results['recall'].append(recall) results['fmeasure'].append(fmeasure) print('precision', results['precision']) print('recall', results['recall']) print('fmeasure', results['fmeasure']) Output yang akan dihasilkan adalah berikut: precision [0.5714285714285714, 0.5714285714285714] recall [0.6666666666666666, 0.6666666666666666] fmeasure [0.6153846153846153, 0.6153846153846153]","title":"Web Mining A 2021"},{"location":"#web-mining-a-2021","text":"Nama : Prita Dyah Anindhita NPM : 180411100125 Dosen Pengampu Mata Kuliah : Mula'ab, S.Si., M.Kom.","title":"Web Mining A 2021"},{"location":"#crawling","text":"","title":"Crawling"},{"location":"#teori-tentang-crawling","text":"Kita semua tahu bahwa dalam website terkandung informasi atau data. Dalam web mining, crawling adalah suatu proses ekstraksi data dari suatu website. Artinya, kita dapat menghimpun data atau informasi tersebut baik secara manual maupun otomatis. Biasanya, crawling digunakan untuk mengambil data dalam jumlah yang banyak dan tidak mungkin untuk disalin satu persatu secara manual. Crawling data dapat dilakukan dengan berbagai cara dan bahasa pemrograman yang beragam pula. Dalam penjelasan kali ini, saya menggunakan bahasa pemrograman Python dengan library Scrapy untuk menghimpun data dari suatu web e-commerce. Lihat disini untuk dokumentasi Scrapy Python.","title":"Teori tentang Crawling"},{"location":"#implementasi-crawling","text":"Contoh crawling yang akan saya lakukan adalah mengambil data produk dari League World yang merupakan website penjualan pakaian dan perlengkapan olahraga. Pertama, tentukan terlebih dahulu page/halaman mana yang akan diambil datanya (dalam hal ini, saya mengambil data produk women collection). Mulai project melalui command prompt C:\\Users> scrapy startproject tugas_scrape Masuk ke dalam direktori project yang telah dibuat C:\\Users> cd tugas_scrape Memasukkan link website yang dituju C:\\Users\\tugas_scrape> scrapy genspider league https://www.league-world.com/ Setelah itu, buka folder project yang telah dibuat. Akan terdapat folder dengan nama spiders berisi file init dan file dengan nama league. Edit pada teks editor python. # -*- coding: utf-8 -*- import scrapy class LeagueSpider(scrapy.Spider): name = 'league' allowed_domains = ['https://www.league-world.com/'] start_urls = ['https://www.league-world.com/tag/29/women/'] def parse(self, response): data = response.css('.product-item') for item in data: nama = item.css('.product-name a::text').extract() harga = item.css('.autoNumeric::text').extract() yield{ 'Nama Barang' : nama, 'Harga Barang' : harga } Dalam kode diatas, saya mengekstrak nama produk dan harga dari halaman produk wanita. Untuk mengambil data nama barang dan harga, saya menggunakan bantuan css seperti yang terdapat pada kode program. Untuk mengekstrak data dari web page tersebut, gunakan perintah pada command prompt sebagai berikut: C:\\Users\\tugas_scrape> scrapy crawl league Untuk menyimpan data hasil crawling ke dalam bentuk file lain (saya simpan dalam bentuk file berekstensi csv yang sudah disiapkan sebelumnya), dapat menggunakan perintah berikut: C:\\Users\\tugas_scrape> scrapy crawl league -o hasil.csv","title":"Implementasi Crawling"},{"location":"#text-preprocessing","text":"Text preprocessing dapat kita artikan sebagai proses untuk menyiapkan teks sebelum dimodelkan lebih lanjut sesuai kebutuhan. Text preprocessing terdiri dari beberapa hal seperti case folding, tokenizing, stemming, stopword removal, dll. Text preprocessing biasanya juga digunakan dalam peringkasan dokumen. Tahapan-tahapan seperti case folding, tokenizing, stemming, stopword removing, dan pembentukan matriks kata sangat membantu dalam meringkas dokumen.","title":"Text Preprocessing"},{"location":"#case-folding","text":"","title":"Case Folding"},{"location":"#teori-tentang-case-folding","text":"Case folding merupakan bagian dari text preprocessing untuk 'merapikan' teks yang akan dimodelkan. Teks selalu ditulis dalam aturan yang umum seperti penggunaan huruf kapital, adanya penggunaan angka ataupun tanda baca dan simbol-simbol. Peran case folding disini adalah mengubah seluruh huruf dalam teks menjadi huruf kecil, menghilangkan angka, dan menghapus tanda baca sehingga teks menjadi lebih rapi untuk diproses ke tahap selanjutnya.","title":"Teori tentang Case Folding"},{"location":"#implementasi-case-folding","text":"","title":"Implementasi Case Folding"},{"location":"#mengubah-seluruh-huruf-dalam-teks-menjadi-huruf-kecil","text":"Untuk mengubah huruf menjadi lowercase seluruhnya, dapat menggunakan perintah bawaan dari python yaitu .lower() teks = \"Prita adalah mahasiswa tahun ke-3 Program Studi S1 Teknik Informatika di Universitas Trunojoyo Madura. Karena masih situasi pandemi, Prita menempuh perkuliahan dari rumah secara daring.\" teks_baru = teks.lower() print(teks_baru) Output akan seperti berikut ini: prita adalah mahasiswa tahun ke-3 program studi s1 teknik informatika di universitas trunojoyo madura. karena masih situasi pandemi, prita menempuh perkuliahan dari rumah secara daring.","title":"Mengubah seluruh huruf dalam teks menjadi huruf kecil"},{"location":"#menghilangkan-angka-dalam-teks","text":"Untuk menghilangkan angka yang ada di dalam teks, dapat menggunakan perintah bawaan dari python sebagai berikut: import re teks = \"Prita adalah mahasiswa tahun ke-3 Program Studi S1 Teknik Informatika di Universitas Trunojoyo Madura. Karena masih situasi pandemi, Prita menempuh perkuliahan dari rumah secara daring.\" teks_baru = re.sub(r\"\\d+\", \"\", teks) print(teks_baru) Output akan seperti berikut ini: Prita adalah mahasiswa tahun ke- Program Studi S Teknik Informatika di Universitas Trunojoyo Madura. Karena masih situasi pandemi, Prita menempuh perkuliahan dari rumah secara daring.","title":"Menghilangkan angka dalam teks"},{"location":"#menghilangkan-tanda-baca-dalam-teks","text":"Untuk menghapus tanda baca yang ada di dalam teks, dapat menggunakan kode sebagai berikut: import string teks = \"Prita adalah mahasiswa tahun ke-3 Program Studi S1 Teknik Informatika di Universitas Trunojoyo Madura. Karena masih situasi pandemi, Prita menempuh perkuliahan dari rumah secara daring.\" teks_baru = teks.translate(str.maketrans(\"\",\"\",string.punctuation)) print(teks_baru) Output akan seperti berikut ini: Prita adalah mahasiswa tahun ke3 Program Studi S1 Teknik Informatika di Universitas Trunojoyo Madura Karena masih situasi pandemi Prita menempuh perkuliahan dari rumah secara daring","title":"Menghilangkan tanda baca dalam teks"},{"location":"#tokenisasi","text":"","title":"Tokenisasi"},{"location":"#teori-tentang-tokenisasi","text":"Tokenizing atau tokenisasi dapat diartikan sebagai proses pemisahan kata-kata dalam suatu teks. Kata demi kata tersebut dipisahkan dan dikenal sebagai token. Dalam Python sendiri, ada syntax yang dapat digunakan untuk memisahkan kata dalam suatu kalimat/paragraf yaitu perintah split(). Namun biasanya dalam suatu paragraf terdapat kata yang berulang kali digunakan. Penggunaan perintah split() tidak lagi dapat membantu karena perintah tersebut hanya memisahkan kata tanpa mengenali apakah kata tersebut sudah ada sebelumnya atau belum. Untuk itu, ada modul NLTK milik Python yang memang ditujukan untuk Text Preprocesing. Perintah yang digunakan adalah word_tokenize()","title":"Teori tentang Tokenisasi"},{"location":"#implementasi-tokenisasi","text":"import nltk from nltk.tokenize import word_tokenize from nltk.probability import FreqDist import string teks = \"Prita adalah mahasiswa tahun ke-3 Program Studi Teknik Informatika di Universitas Trunojoyo Madura. Sebelum pandemi terjadi, biasanya Prita berangkat dari rumah ke kos menaiki bus. Setelah pandemi ada di Indonesia, Prita sama sekali belum kembali ke kos.\" teks = teks.translate(str.maketrans('','',string.punctuation)) kata = nltk.tokenize.word_tokenize(teks) frekuensi = nltk.FreqDist(kata) print(frekuensi.most_common()) Output akan seperti berikut ini: [('Prita', 3), ('di', 2), ('pandemi', 2), ('ke', 2), ('kos', 2), ('adalah', 1), ('mahasiswa', 1), ('tahun', 1), ('ke3', 1), ('Program', 1), ('Studi', 1), ('Teknik', 1), ('Informatika', 1), ('Universitas', 1), ('Trunojoyo', 1), ('Madura', 1), ('Sebelum', 1), ('terjadi', 1), ('biasanya', 1), ('berangkat', 1), ('dari', 1), ('rumah', 1), ('menaiki', 1), ('bus', 1), ('Setelah', 1), ('ada', 1), ('Indonesia', 1), ('sama', 1), ('sekali', 1), ('belum', 1), ('kembali', 1)]","title":"Implementasi Tokenisasi"},{"location":"#stemming","text":"","title":"Stemming"},{"location":"#teori-tentang-stemming","text":"Stemming adalah suatu proses untuk mengidentifikasi kata dasar dari suatu kata yang ada. Misalnya, ada kata \"melihat\" maka ketika dilakukan proses stemming, akan diperoleh kata dasar yaitu \"lihat\". Dalam Python, untuk melakukan stemming dapat menggunakan library NLTK, namun ada library khusus yang diciptakan untuk stemming kata dalam bahasa Indonesia yaitu Python Sastrawi.","title":"Teori tentang Stemming"},{"location":"#implementasi-stemming","text":"from Sastrawi.Stemmer.StemmerFactory import StemmerFactory factory = StemmerFactory() stemmer = factory.create_stemmer() teks = \"Prita selalu melakukan olahraga setiap hari selama satu jam. Menurut Prita, olahraga mampu membantunya memperbaiki pola tidur di malam hari.\" teks_baru = stemmer.stem(teks) print(teks_baru) Outputnya akan seperti ini: prita selalu laku olahraga tiap hari lama satu jam turut prita olahraga mampu bantu baik pola tidur di malam hari","title":"Implementasi Stemming"},{"location":"#stopword-removal","text":"","title":"Stopword Removal"},{"location":"#teori-tentang-stopword","text":"Dalam sebuah teks yang panjang, seringkali terdapat kata yang tidak terlalu penting seperti kata hubung \"di\", \"ke\", \"yang\" dan lain sebagainya. Kata-kata tersebut sebaiknya tidak terlalu diperhatikan karena memang tidak penting seperti kata lain yang mungkin saja menjadi kunci dari suatu teks. Kata-kata yang tidak penting tersebut dikenal sebagai stopword.","title":"Teori tentang Stopword"},{"location":"#implementasi-stopword-removal","text":"from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from nltk.tokenize import word_tokenize factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover() teks = \"Dilaporkan pada Selasa, 8 Juni 2021 terjadi kecelakaan tunggal di Jalan Pattimura. Korban adalah siswa SMA bernama Doni yang baru pulang sekolah. Korban mengaku mengantuk sehingga menabrak pohon di tepi jalan. Kondisi korban saat ini baik-baik saja dan hanya mengalami luka ringan.\" teks_baru = stopword.remove(teks) print(teks_baru) Output akan seperti ini: Dilaporkan Selasa, 8 Juni 2021 terjadi kecelakaan tunggal Jalan Pattimura. Korban siswa SMA bernama Doni baru pulang sekolah. Korban mengaku mengantuk menabrak pohon tepi jalan. Kondisi korban ini baik-baik dan mengalami luka ringan.","title":"Implementasi Stopword Removal"},{"location":"#reduksi-dimensi","text":"","title":"Reduksi Dimensi"},{"location":"#teori","text":"Dalam pengolahan dokumen yang berbasis teks, tentu saja terdapat banyak kosakata. Ketika beragam kosakata tersebut berjumlah sangat banyak, maka diperlukan reduksi dimensi berupa seleksi fitur. Fitur perlu diseleksi untuk menghindari kemunculan berulang suatu fitur dan hanya memilih fitur-fitur yang penting saja. Reduksi dimensi dengan melakukan seleksi fitur dapat diterapkan dengan menggunakan matriks TF-IDF (Term Frequency-Inverse Document Frequency). Dalam matriks TF-IDF, kolom dan barisnya didasarkan pada kosakata dan jumlah kalimat dalam dokumen. Isinya sendiri merupakan frekuensi kemunculan kata dalam setiap potong kalimat. Setelah terbentuk matriks, baru dilakukan reduksi dimensi.","title":"Teori"},{"location":"#implementasi","text":"Untuk membentuk matriks TF-IDF, dapat menggunakan library sklearn dari Python dan mengimport modul TfidfTransformer. Lalu untuk reduksi dimensi menggunakan PCA. Dokumen harus melalui case folding, stemming, tokenizing, dan stopword removal untuk sampai ke tahap ini. Berikut adalah tampilan dari matriks TFIDF Untuk mereduksi dimensi, gunakan code berikut: from sklearn.decomposition import PCA pca = PCA (n_components=2) fit_pca = pca.fit_transform(dfb) pca_df = pd.DataFrame(data=fit_pca, columns=['PCA 1', 'PCA 2']) pca_df.tail() Output yang dihasilkan: PCA 1 PCA 2 4 -0.019008 -0.278487 5 -0.291685 -0.128199 6 -0.052512 0.532792 7 0.251088 0.735093 8 0.726751 -0.208282","title":"Implementasi"},{"location":"#implementasi-text-preprocessing","text":"import numpy as np import PyPDF2 import doctest import sys from IPython.display import Image from requests_html import HTMLSession import matplotlib.pyplot as plt %matplotlib inline import networkx as nx from nltk.tokenize.punkt import PunktSentenceTokenizer from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer session = HTMLSession() r = session.get('https://suarabojonegoro.com/news/2021/03/15/kapolres-bojonegoro-beri-penghargaan-kepada-kades-kasun-dan-ketua-rukun-tetangga-telah-bantu-percepatan-tangani-covid-19') articles = r.html.find('div.entry-content') for item in articles: newsitem = item.find('div.entry-content-single', first=True) news = newsitem.text print(news) doc_tokenizer = PunktSentenceTokenizer() sentences_list = doc_tokenizer.tokenize(news) type(sentences_list) import string from Sastrawi.Stemmer.StemmerFactory import StemmerFactory# create stemmer factory = StemmerFactory() stemmer = factory.create_stemmer() import re dokumenre=[] for i in sentences_list: hasil = re.sub(r\"\\d+\", \"\", i) dokumenre.append(hasil) print(dokumenre) len(dokumenre) dokumen=[] for i in dokumenre: hasil = i.replace('\\n','') dokumen.append(hasil) print(dokumen) from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from nltk.tokenize import word_tokenize factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover() #a=cv_matrix.toarray() a=len(dokumen) dokumenstop=[] for i in range(0, a): sentence = stopword.remove(dokumen[i]) dokumenstop.append(sentence) print(dokumenstop) cv = CountVectorizer() cv_matrix = cv.fit_transform(dokumen) a=cv_matrix.toarray() a.shape from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from nltk.tokenize import word_tokenize factory = StopWordRemoverFactory() dokumenstop=[] for i in dokumen: output = i.translate(str.maketrans(\"\",\"\",string.punctuation)) dokumenstop.append(output) print(dokumenstop) factory = StemmerFactory() stemmer = factory.create_stemmer() dokumenstem=[] for i in dokumenstop: output = stemmer.stem(i) dokumenstem.append(output) print(dokumenstem) bag = cv.fit_transform(dokumenstem) print(cv.vocabulary_) print(cv.get_feature_names()) len(cv.get_feature_names()) bag = cv.fit_transform(dokumenstem) matrik_vsm=bag.toarray() matrik_vsm.shape matrik_vsm[0] import pandas as pd a=cv.get_feature_names() print(len(matrik_vsm[:,1])) print(len(matrik_vsm[:,1])) dfb =pd.DataFrame(data=matrik_vsm,index=list(range(1, len(matrik_vsm[:,1])+1, )),columns=[a]) dfb from sklearn.feature_extraction.text import TfidfTransformer tfidf = TfidfTransformer(use_idf=True,norm='l2',smooth_idf=True) tf=tfidf.fit_transform(cv.fit_transform(dokumenstem)).toarray() dfb =pd.DataFrame(data=tf,index=list(range(1, len(tf[:,1])+1, )),columns=[a]) dfb","title":"Implementasi Text Preprocessing"},{"location":"#modelling","text":"","title":"Modelling"},{"location":"#teori_1","text":"Text classification merupakan salah satu hal dalam pengolahan bahasa alami. Biasanya, text classification digunakan dalam analisis sentimen (bisa diterapkan untuk analisis sentimen media sosial), pelabelan topik, dan lain-lain.","title":"Teori"},{"location":"#implementasi_1","text":"Implementasi text classification melibatkan text preprocessing, maka saya akan memberikan contoh code untuk meringkas dokumen menggunakan metode LSA (Latent Semantic Analysis) pada teks yang telah melalui preprocessing. doc_complete = ['roni pergi rumah nenek desa', 'sana banyak pohon hijau sejuk teduh'] doc_complete from sklearn.feature_extraction.text import TfidfVectorizer vectorizer = TfidfVectorizer() X =vectorizer.fit_transform(doc_complete) print(X) from sklearn.decomposition import TruncatedSVD lsa = TruncatedSVD(n_components=2,n_iter=100) lsa.fit(X) terms = vectorizer.get_feature_names() for i,comp in enumerate(lsa.components_): termsInComp = zip(terms,comp) sortedterms = sorted(termsInComp, key=lambda x: x[1],reverse=True)[:10] print(\"Concept %d:\" % i) for term in sortedterms: print(term[0]) print(\" \") Output program: Concept 0: desa nenek pergi roni rumah hijau pohon sana sejuk teduh Concept 1: hijau pohon sana sejuk teduh banyak desa nenek pergi ron","title":"Implementasi"},{"location":"#evaluasi","text":"","title":"Evaluasi"},{"location":"#teori_2","text":"Evaluasi ringkasan otomatis dapat dilakukan dengan banyak metode. Yang paling sering digunakan adalah ROUGE-n. ROUGE sendiri merupakan seperangkat metrik untuk mengevaluasi ringkasan teks otomatis yang bekerja dengan cara membandingkan suatu ringkasan otomatis atau terjemahan dengan seperangkat rangkuman referensi. Pengukuran ROUGE terbagi menjadi beberapa: ROUGE-N yang digunakan untuk mengukur unigram, bigram, trigram, dan n-gram yang lebih tinggi lagi ROUGE-L yang digunakan untuk mengukur pencocokan kata terpanjang menggunakan LCS. ROUGE-S yang juga bisa disebut sebagai skip-gram cooccurence. Misalnya, skip-bigram mengukur overlap dari pasangan kata yang memiliki jumlah gaps maksimum sebanyak dua pada setiap kata. ROUGE-SU merupakan pengembangan dari ROUGE-S yang memperhatikan unigram. ROUGE-W yang menggunakan LCS namun tidak memberikan preferensi pada kalimat yang emiliki kata-kata yang lebih berurutan. Untuk mengukur keakuratan ringkasan harus menghitung Precision, Recall, dan F-Measure. Rumus menghitung recall adalah banyaknya kata yang overlap dibagi dengan banyaknya kata pada ringkasan rujukan. Rumus menghitung Precision adalah banyaknya kata yang overlap dibagi dengan banyaknya kata pada ringkasan mesin Rumus menghitung F-Measure adalah","title":"Teori"},{"location":"#implementasi_2","text":"from rouge_score import rouge_scorer hypotesis = ['Godric Gryffindor is the bravest wizard', 'Salazar Slytherin is the smartest wizard'] reference = ['Gryffindor house is for the bravest one', 'Slytherin house is for the smartest one'] scorer = rouge_scorer.RougeScorer(['rouge1']) results = {'precision': [], 'recall': [], 'fmeasure': []} for (h, r) in zip(hypotesis, reference): score = scorer.score(h, r) precision, recall, fmeasure = score['rouge1'] results['precision'].append(precision) results['recall'].append(recall) results['fmeasure'].append(fmeasure) print('precision', results['precision']) print('recall', results['recall']) print('fmeasure', results['fmeasure']) Output yang akan dihasilkan adalah berikut: precision [0.5714285714285714, 0.5714285714285714] recall [0.6666666666666666, 0.6666666666666666] fmeasure [0.6153846153846153, 0.6153846153846153]","title":"Implementasi"}]}
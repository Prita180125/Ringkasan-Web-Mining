{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Web Mining A 2021 Nama : Prita Dyah Anindhita NPM : 180411100125 Dosen Pengampu Mata Kuliah : Mula'ab, S.Si., M.Kom. Crawling Teori tentang Crawling Kita semua tahu bahwa dalam website terkandung informasi atau data. Dalam web mining, crawling adalah suatu proses ekstraksi data dari suatu website. Artinya, kita dapat menghimpun data atau informasi tersebut baik secara manual maupun otomatis. Biasanya, crawling digunakan untuk mengambil data dalam jumlah yang banyak dan tidak mungkin untuk disalin satu persatu secara manual. Crawling data dapat dilakukan dengan berbagai cara dan bahasa pemrograman yang beragam pula. Dalam penjelasan kali ini, saya menggunakan bahasa pemrograman Python dengan library Scrapy untuk menghimpun data dari suatu web e-commerce. Lihat disini untuk dokumentasi Scrapy Python. Implementasi Crawling Contoh crawling yang akan saya lakukan adalah mengambil data produk dari League World yang merupakan website penjualan pakaian dan perlengkapan olahraga. Pertama, tentukan terlebih dahulu page/halaman mana yang akan diambil datanya (dalam hal ini, saya mengambil data produk women collection). Mulai project melalui command prompt C:\\Users> scrapy startproject tugas_scrape Masuk ke dalam direktori project yang telah dibuat C:\\Users> cd tugas_scrape Memasukkan link website yang dituju C:\\Users\\tugas_scrape> scrapy genspider league https://www.league-world.com/ Setelah itu, buka folder project yang telah dibuat. Akan terdapat folder dengan nama spiders berisi file init dan file dengan nama league. Edit pada teks editor python. # -*- coding: utf-8 -*- import scrapy class LeagueSpider(scrapy.Spider): name = 'league' allowed_domains = ['https://www.league-world.com/'] start_urls = ['https://www.league-world.com/tag/29/women/'] def parse(self, response): data = response.css('.product-item') for item in data: nama = item.css('.product-name a::text').extract() harga = item.css('.autoNumeric::text').extract() yield{ 'Nama Barang' : nama, 'Harga Barang' : harga } Dalam kode diatas, saya mengekstrak nama produk dan harga dari halaman produk wanita. Untuk mengambil data nama barang dan harga, saya menggunakan bantuan css seperti yang terdapat pada kode program. Untuk mengekstrak data dari web page tersebut, gunakan perintah pada command prompt sebagai berikut: C:\\Users\\tugas_scrape> scrapy crawl league Untuk menyimpan data hasil crawling ke dalam bentuk file lain (saya simpan dalam bentuk file berekstensi csv yang sudah disiapkan sebelumnya), dapat menggunakan perintah berikut: C:\\Users\\tugas_scrape> scrapy crawl league -o hasil.csv Text Preprocessing Text preprocessing dapat kita artikan sebagai proses untuk menyiapkan teks sebelum dimodelkan lebih lanjut sesuai kebutuhan. Text preprocessing terdiri dari beberapa hal seperti case folding, tokenizing, stemming, stopword removal, dll. Text preprocessing biasanya juga digunakan dalam peringkasan dokumen. Tahapan-tahapan seperti case folding, tokenizing, stemming, stopword removing, dan pembentukan matriks kata sangat membantu dalam meringkas dokumen. Case Folding Teori tentang Case Folding Case folding merupakan bagian dari text preprocessing untuk 'merapikan' teks yang akan dimodelkan. Teks selalu ditulis dalam aturan yang umum seperti penggunaan huruf kapital, adanya penggunaan angka ataupun tanda baca dan simbol-simbol. Peran case folding disini adalah mengubah seluruh huruf dalam teks menjadi huruf kecil, menghilangkan angka, dan menghapus tanda baca sehingga teks menjadi lebih rapi untuk diproses ke tahap selanjutnya. Implementasi Case Folding Mengubah seluruh huruf dalam teks menjadi huruf kecil Untuk mengubah huruf menjadi lowercase seluruhnya, dapat menggunakan perintah bawaan dari python yaitu .lower() teks = \"Prita adalah mahasiswa tahun ke-3 Program Studi S1 Teknik Informatika di Universitas Trunojoyo Madura. Karena masih situasi pandemi, Prita menempuh perkuliahan dari rumah secara daring.\" teks_baru = teks.lower() print(teks_baru) Output akan seperti berikut ini: prita adalah mahasiswa tahun ke-3 program studi s1 teknik informatika di universitas trunojoyo madura. karena masih situasi pandemi, prita menempuh perkuliahan dari rumah secara daring. Menghilangkan angka dalam teks Untuk menghilangkan angka yang ada di dalam teks, dapat menggunakan perintah bawaan dari python sebagai berikut: import re teks = \"Prita adalah mahasiswa tahun ke-3 Program Studi S1 Teknik Informatika di Universitas Trunojoyo Madura. Karena masih situasi pandemi, Prita menempuh perkuliahan dari rumah secara daring.\" teks_baru = re.sub(r\"\\d+\", \"\", teks) print(teks_baru) Output akan seperti berikut ini: Prita adalah mahasiswa tahun ke- Program Studi S Teknik Informatika di Universitas Trunojoyo Madura. Karena masih situasi pandemi, Prita menempuh perkuliahan dari rumah secara daring. Menghilangkan tanda baca dalam teks Untuk menghapus tanda baca yang ada di dalam teks, dapat menggunakan kode sebagai berikut: import string teks = \"Prita adalah mahasiswa tahun ke-3 Program Studi S1 Teknik Informatika di Universitas Trunojoyo Madura. Karena masih situasi pandemi, Prita menempuh perkuliahan dari rumah secara daring.\" teks_baru = teks.translate(str.maketrans(\"\",\"\",string.punctuation)) print(teks_baru) Output akan seperti berikut ini: Prita adalah mahasiswa tahun ke3 Program Studi S1 Teknik Informatika di Universitas Trunojoyo Madura Karena masih situasi pandemi Prita menempuh perkuliahan dari rumah secara daring Tokenisasi Teori tentang Tokenisasi Tokenizing atau tokenisasi dapat diartikan sebagai proses pemisahan kata-kata dalam suatu teks. Kata demi kata tersebut dipisahkan dan dikenal sebagai token. Dalam Python sendiri, ada syntax yang dapat digunakan untuk memisahkan kata dalam suatu kalimat/paragraf yaitu perintah split(). Namun biasanya dalam suatu paragraf terdapat kata yang berulang kali digunakan. Penggunaan perintah split() tidak lagi dapat membantu karena perintah tersebut hanya memisahkan kata tanpa mengenali apakah kata tersebut sudah ada sebelumnya atau belum. Untuk itu, ada modul NLTK milik Python yang memang ditujukan untuk Text Preprocesing. Perintah yang digunakan adalah word_tokenize() Implementasi Tokenisasi import nltk from nltk.tokenize import word_tokenize from nltk.probability import FreqDist import string teks = \"Prita adalah mahasiswa tahun ke-3 Program Studi Teknik Informatika di Universitas Trunojoyo Madura. Sebelum pandemi terjadi, biasanya Prita berangkat dari rumah ke kos menaiki bus. Setelah pandemi ada di Indonesia, Prita sama sekali belum kembali ke kos.\" teks = teks.translate(str.maketrans('','',string.punctuation)) kata = nltk.tokenize.word_tokenize(teks) frekuensi = nltk.FreqDist(kata) print(frekuensi.most_common()) Output akan seperti berikut ini: [('Prita', 3), ('di', 2), ('pandemi', 2), ('ke', 2), ('kos', 2), ('adalah', 1), ('mahasiswa', 1), ('tahun', 1), ('ke3', 1), ('Program', 1), ('Studi', 1), ('Teknik', 1), ('Informatika', 1), ('Universitas', 1), ('Trunojoyo', 1), ('Madura', 1), ('Sebelum', 1), ('terjadi', 1), ('biasanya', 1), ('berangkat', 1), ('dari', 1), ('rumah', 1), ('menaiki', 1), ('bus', 1), ('Setelah', 1), ('ada', 1), ('Indonesia', 1), ('sama', 1), ('sekali', 1), ('belum', 1), ('kembali', 1)] Stemming Teori tentang Stemming Stemming adalah suatu proses untuk mengidentifikasi kata dasar dari suatu kata yang ada. Misalnya, ada kata \"melihat\" maka ketika dilakukan proses stemming, akan diperoleh kata dasar yaitu \"lihat\". Dalam Python, untuk melakukan stemming dapat menggunakan library NLTK, namun ada library khusus yang diciptakan untuk stemming kata dalam bahasa Indonesia yaitu Python Sastrawi. Implementasi Stemming from Sastrawi.Stemmer.StemmerFactory import StemmerFactory factory = StemmerFactory() stemmer = factory.create_stemmer() teks = \"Prita selalu melakukan olahraga setiap hari selama satu jam. Menurut Prita, olahraga mampu membantunya memperbaiki pola tidur di malam hari.\" teks_baru = stemmer.stem(teks) print(teks_baru) Outputnya akan seperti ini: prita selalu laku olahraga tiap hari lama satu jam turut prita olahraga mampu bantu baik pola tidur di malam hari Stopword Removal Teori tentang Stopword Dalam sebuah teks yang panjang, seringkali terdapat kata yang tidak terlalu penting seperti kata hubung \"di\", \"ke\", \"yang\" dan lain sebagainya. Kata-kata tersebut sebaiknya tidak terlalu diperhatikan karena memang tidak penting seperti kata lain yang mungkin saja menjadi kunci dari suatu teks. Kata-kata yang tidak penting tersebut dikenal sebagai stopword. Implementasi Stopword Removal from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from nltk.tokenize import word_tokenize factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover() teks = \"Dilaporkan pada Selasa, 8 Juni 2021 terjadi kecelakaan tunggal di Jalan Pattimura. Korban adalah siswa SMA bernama Doni yang baru pulang sekolah. Korban mengaku mengantuk sehingga menabrak pohon di tepi jalan. Kondisi korban saat ini baik-baik saja dan hanya mengalami luka ringan.\" teks_baru = stopword.remove(teks) print(teks_baru) Output akan seperti ini: Dilaporkan Selasa, 8 Juni 2021 terjadi kecelakaan tunggal Jalan Pattimura. Korban siswa SMA bernama Doni baru pulang sekolah. Korban mengaku mengantuk menabrak pohon tepi jalan. Kondisi korban ini baik-baik dan mengalami luka ringan. Reduksi Dimensi Teori Dalam pengolahan dokumen yang berbasis teks, tentu saja terdapat banyak kosakata. Ketika beragam kosakata tersebut berjumlah sangat banyak, maka diperlukan reduksi dimensi berupa seleksi fitur. Fitur perlu diseleksi untuk menghindari kemunculan berulang suatu fitur dan hanya memilih fitur-fitur yang penting saja. Reduksi dimensi dengan melakukan seleksi fitur dapat diterapkan dengan menggunakan matriks TF-IDF (Term Frequency-Inverse Document Frequency) Implementasi Untuk membentuk matriks TF-IDF, dapat menggunakan library sklearn dari Python dan mengimport modul TfidfTransformer. Implementasi Text Preprocessing import numpy as np import PyPDF2 import doctest import sys from IPython.display import Image from requests_html import HTMLSession import matplotlib.pyplot as plt %matplotlib inline import networkx as nx from nltk.tokenize.punkt import PunktSentenceTokenizer from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer session = HTMLSession() r = session.get('https://suarabojonegoro.com/news/2021/03/15/kapolres-bojonegoro-beri-penghargaan-kepada-kades-kasun-dan-ketua-rukun-tetangga-telah-bantu-percepatan-tangani-covid-19') articles = r.html.find('div.entry-content') for item in articles: newsitem = item.find('div.entry-content-single', first=True) news = newsitem.text print(news) doc_tokenizer = PunktSentenceTokenizer() sentences_list = doc_tokenizer.tokenize(news) type(sentences_list) import string from Sastrawi.Stemmer.StemmerFactory import StemmerFactory# create stemmer factory = StemmerFactory() stemmer = factory.create_stemmer() import re dokumenre=[] for i in sentences_list: hasil = re.sub(r\"\\d+\", \"\", i) dokumenre.append(hasil) print(dokumenre) len(dokumenre) dokumen=[] for i in dokumenre: hasil = i.replace('\\n','') dokumen.append(hasil) print(dokumen) from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from nltk.tokenize import word_tokenize factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover() #a=cv_matrix.toarray() a=len(dokumen) dokumenstop=[] for i in range(0, a): sentence = stopword.remove(dokumen[i]) dokumenstop.append(sentence) print(dokumenstop) cv = CountVectorizer() cv_matrix = cv.fit_transform(dokumen) a=cv_matrix.toarray() a.shape from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from nltk.tokenize import word_tokenize factory = StopWordRemoverFactory() dokumenstop=[] for i in dokumen: output = i.translate(str.maketrans(\"\",\"\",string.punctuation)) dokumenstop.append(output) print(dokumenstop) factory = StemmerFactory() stemmer = factory.create_stemmer() dokumenstem=[] for i in dokumenstop: output = stemmer.stem(i) dokumenstem.append(output) print(dokumenstem) bag = cv.fit_transform(dokumenstem) print(cv.vocabulary_) print(cv.get_feature_names()) len(cv.get_feature_names()) bag = cv.fit_transform(dokumenstem) matrik_vsm=bag.toarray() matrik_vsm.shape matrik_vsm[0] import pandas as pd a=cv.get_feature_names() print(len(matrik_vsm[:,1])) print(len(matrik_vsm[:,1])) dfb =pd.DataFrame(data=matrik_vsm,index=list(range(1, len(matrik_vsm[:,1])+1, )),columns=[a]) dfb from sklearn.feature_extraction.text import TfidfTransformer tfidf = TfidfTransformer(use_idf=True,norm='l2',smooth_idf=True) tf=tfidf.fit_transform(cv.fit_transform(dokumenstem)).toarray() dfb =pd.DataFrame(data=tf,index=list(range(1, len(tf[:,1])+1, )),columns=[a]) dfb Modelling Teori Text classification merupakan salah satu hal dalam pengolahan bahasa alami. Biasanya, text classification digunakan dalam analisis sentimen (bisa diterapkan untuk analisis sentimen media sosial), pelabelan topik, dan lain-lain. Implementasi Implementasi text classification melibatkan text preprocessing, maka saya akan memberikan contoh code untuk meringkas dokumen menggunakan metode LSA (Latent Semantic Analysis). Dokumen yang saya gunakan adalah berita dari media online yang linknya tertera pada code berikut: import codecs import string import operator import numpy as np from nltk.tokenize import sent_tokenize from nltk.corpus import stopwords from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.utils.extmath import randomized_svd from requests_html import HTMLSession from Sastrawi.Stemmer.StemmerFactory import StemmerFactory# create stemmer from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory #untuk stopword remove from nltk.tokenize.punkt import PunktSentenceTokenizer session = HTMLSession() r = session.get('https://suarabojonegoro.com/news/2021/02/27/10-orang-terjaring-razia-protokol-kesehatan-covid-19-di-bojonegoro') articles = r.html.find('div.entry-content') for item in articles: newsitem = item.find('div.entry-content-single', first=True) news = newsitem.text print(news) sent_tokenize_list = sent_tokenize(news) stopwords= set(StopWordRemoverFactory().get_stop_words()) #Menghitung TF-IDF vectorizer = TfidfVectorizer(stop_words=stopwords, use_idf=True, ngram_range = (1, 3)) X = vectorizer.fit_transform(sent_tokenize_list) X_T = np.transpose(X) U, Sigma, VT = randomized_svd(X_T, n_components=100, n_iter=100, random_state=None) k = 4 temp_k = k i = 0 index = 0 output = [] index_list = [] if temp_k <= 4: dic = {} for j in range(0, len(VT[0])): dic[j] = VT[0][j] dic_sort = sorted(dic.items(), key=operator.itemgetter(1)) index1 = dic_sort[-1][0] index2 = dic_sort[-2][0] index3 = dic_sort[-3][0] index4 = dic_sort[-4][0] list = [index1, index2,index3,index4] list = sorted(list) if k == 1: output.append(sent_tokenize_list[list[0]]) if k == 2: output.append(sent_tokenize_list[list[0]]) output.append(sent_tokenize_list[list[1]]) if k ==3: output.append(sent_tokenize_list[list[0]]) output.append(sent_tokenize_list[list[1]]) output.append(sent_tokenize_list[list[2]]) if k ==4: output.append(sent_tokenize_list[list[0]]) output.append(sent_tokenize_list[list[1]]) output.append(sent_tokenize_list[list[2]]) output.append(sent_tokenize_list[list[3]]) summarized_text = \" \".join(output) print(\"Hasil rangkuman: \\n\",summarized_text) Output program adalah hasil ringkasan dari berita sebagai berikut: Hasil rangkuman: Reporter: Yudianto SuaraBojonegoro.com \u2013 Sebanyak 10 orang warga terjaring operasi Yustisi Protokol Kesehatan Covid 19 yang dilakukan oleh Petugas Gabungan, Dari Polres Bojonegoro, Kodim Bojonegoro, dan Satpol PP Pemkab Bojonegoro, Sabtu (27/2/2021). (adsbygoogle = window.adsbygoogle || []).push({}); \u201cMereka pelanggar Covid 19 ini disangsi karena diketahui tidak mengunakan masker,\u201d Kata Benny. Dalam Kegiatan yang dilaksanakan Monitor Bangsit Pendisiplinan Prokes Covid 19 juga Melakukan Himbauan kepada Masyarakat agar mematuhi Protokol Kesehatan Covid-19. (Yud/Red) (adsbygoogle = window.adsbygoogle || []).push({}); Evaluasi Teori Evaluasi ringkasan otomatis dapat dilakukan dengan banyak metode. Yang paling sering digunakan adalah ROUGE-n. ROUGE sendiri merupakan seperangkat metrik untuk mengevaluasi ringkasan teks otomatis yang bekerja dengan cara membandingkan suatu ringkasan otomatis atau terjemahan dengan seperangkat rangkuman referensi. Pengukuran ROUGE terbagi menjadi beberapa: ROUGE-N yang digunakan untuk mengukur unigram, bigram, trigram, dan n-gram yang lebih tinggi lagi ROUGE-L yang digunakan untuk mengukur pencocokan kata terpanjang menggunakan LCS. ROUGE-S yang juga bisa disebut sebagai skip-gram cooccurence. Misalnya, skip-bigram mengukur overlap dari pasangan kata yang memiliki jumlah gaps maksimum sebanyak dua pada setiap kata. ROUGE-SU merupakan pengembangan dari ROUGE-S yang memperhatikan unigram. ROUGE-W yang menggunakan LCS namun tidak memberikan preferensi pada kalimat yang emiliki kata-kata yang lebih berurutan. Untuk mengukur keakuratan ringkasan harus menghitung Precision, Recall, dan F-Measure. Rumus menghitung recall adalah banyaknya kata yang overlap dibagi dengan banyaknya kata pada ringkasan rujukan. Rumus menghitung Precision adalah banyaknya kata yang overlap dibagi dengan banyaknya kata pada ringkasan mesin Rumus menghitung F-Measure adalah Implementasi from rouge.rouge import rouge_n_sentence_level summary_sentence = 'the capital of China is Beijing'.split() reference_sentence = 'Beijing is the Capital of China'.split() #Menghitung nilai rouge recall, precision, rouge = rouge_n_sentence_level(summary_senteence, reference_sentence, 2) print ('ROUGE-2-R', recall) print ('ROUGE-2-P', precision) print ('ROUGE-2-F', rouge) Output yang akan dihasilkan adalah berikut: ROUGE-2-R 0.6 ROUGE-2-P 0.6 ROUGE-2-F 0.6 ROUGE-2-R 0.6","title":"Web Mining A 2021"},{"location":"#web-mining-a-2021","text":"Nama : Prita Dyah Anindhita NPM : 180411100125 Dosen Pengampu Mata Kuliah : Mula'ab, S.Si., M.Kom.","title":"Web Mining A 2021"},{"location":"#crawling","text":"","title":"Crawling"},{"location":"#teori-tentang-crawling","text":"Kita semua tahu bahwa dalam website terkandung informasi atau data. Dalam web mining, crawling adalah suatu proses ekstraksi data dari suatu website. Artinya, kita dapat menghimpun data atau informasi tersebut baik secara manual maupun otomatis. Biasanya, crawling digunakan untuk mengambil data dalam jumlah yang banyak dan tidak mungkin untuk disalin satu persatu secara manual. Crawling data dapat dilakukan dengan berbagai cara dan bahasa pemrograman yang beragam pula. Dalam penjelasan kali ini, saya menggunakan bahasa pemrograman Python dengan library Scrapy untuk menghimpun data dari suatu web e-commerce. Lihat disini untuk dokumentasi Scrapy Python.","title":"Teori tentang Crawling"},{"location":"#implementasi-crawling","text":"Contoh crawling yang akan saya lakukan adalah mengambil data produk dari League World yang merupakan website penjualan pakaian dan perlengkapan olahraga. Pertama, tentukan terlebih dahulu page/halaman mana yang akan diambil datanya (dalam hal ini, saya mengambil data produk women collection). Mulai project melalui command prompt C:\\Users> scrapy startproject tugas_scrape Masuk ke dalam direktori project yang telah dibuat C:\\Users> cd tugas_scrape Memasukkan link website yang dituju C:\\Users\\tugas_scrape> scrapy genspider league https://www.league-world.com/ Setelah itu, buka folder project yang telah dibuat. Akan terdapat folder dengan nama spiders berisi file init dan file dengan nama league. Edit pada teks editor python. # -*- coding: utf-8 -*- import scrapy class LeagueSpider(scrapy.Spider): name = 'league' allowed_domains = ['https://www.league-world.com/'] start_urls = ['https://www.league-world.com/tag/29/women/'] def parse(self, response): data = response.css('.product-item') for item in data: nama = item.css('.product-name a::text').extract() harga = item.css('.autoNumeric::text').extract() yield{ 'Nama Barang' : nama, 'Harga Barang' : harga } Dalam kode diatas, saya mengekstrak nama produk dan harga dari halaman produk wanita. Untuk mengambil data nama barang dan harga, saya menggunakan bantuan css seperti yang terdapat pada kode program. Untuk mengekstrak data dari web page tersebut, gunakan perintah pada command prompt sebagai berikut: C:\\Users\\tugas_scrape> scrapy crawl league Untuk menyimpan data hasil crawling ke dalam bentuk file lain (saya simpan dalam bentuk file berekstensi csv yang sudah disiapkan sebelumnya), dapat menggunakan perintah berikut: C:\\Users\\tugas_scrape> scrapy crawl league -o hasil.csv","title":"Implementasi Crawling"},{"location":"#text-preprocessing","text":"Text preprocessing dapat kita artikan sebagai proses untuk menyiapkan teks sebelum dimodelkan lebih lanjut sesuai kebutuhan. Text preprocessing terdiri dari beberapa hal seperti case folding, tokenizing, stemming, stopword removal, dll. Text preprocessing biasanya juga digunakan dalam peringkasan dokumen. Tahapan-tahapan seperti case folding, tokenizing, stemming, stopword removing, dan pembentukan matriks kata sangat membantu dalam meringkas dokumen.","title":"Text Preprocessing"},{"location":"#case-folding","text":"","title":"Case Folding"},{"location":"#teori-tentang-case-folding","text":"Case folding merupakan bagian dari text preprocessing untuk 'merapikan' teks yang akan dimodelkan. Teks selalu ditulis dalam aturan yang umum seperti penggunaan huruf kapital, adanya penggunaan angka ataupun tanda baca dan simbol-simbol. Peran case folding disini adalah mengubah seluruh huruf dalam teks menjadi huruf kecil, menghilangkan angka, dan menghapus tanda baca sehingga teks menjadi lebih rapi untuk diproses ke tahap selanjutnya.","title":"Teori tentang Case Folding"},{"location":"#implementasi-case-folding","text":"","title":"Implementasi Case Folding"},{"location":"#mengubah-seluruh-huruf-dalam-teks-menjadi-huruf-kecil","text":"Untuk mengubah huruf menjadi lowercase seluruhnya, dapat menggunakan perintah bawaan dari python yaitu .lower() teks = \"Prita adalah mahasiswa tahun ke-3 Program Studi S1 Teknik Informatika di Universitas Trunojoyo Madura. Karena masih situasi pandemi, Prita menempuh perkuliahan dari rumah secara daring.\" teks_baru = teks.lower() print(teks_baru) Output akan seperti berikut ini: prita adalah mahasiswa tahun ke-3 program studi s1 teknik informatika di universitas trunojoyo madura. karena masih situasi pandemi, prita menempuh perkuliahan dari rumah secara daring.","title":"Mengubah seluruh huruf dalam teks menjadi huruf kecil"},{"location":"#menghilangkan-angka-dalam-teks","text":"Untuk menghilangkan angka yang ada di dalam teks, dapat menggunakan perintah bawaan dari python sebagai berikut: import re teks = \"Prita adalah mahasiswa tahun ke-3 Program Studi S1 Teknik Informatika di Universitas Trunojoyo Madura. Karena masih situasi pandemi, Prita menempuh perkuliahan dari rumah secara daring.\" teks_baru = re.sub(r\"\\d+\", \"\", teks) print(teks_baru) Output akan seperti berikut ini: Prita adalah mahasiswa tahun ke- Program Studi S Teknik Informatika di Universitas Trunojoyo Madura. Karena masih situasi pandemi, Prita menempuh perkuliahan dari rumah secara daring.","title":"Menghilangkan angka dalam teks"},{"location":"#menghilangkan-tanda-baca-dalam-teks","text":"Untuk menghapus tanda baca yang ada di dalam teks, dapat menggunakan kode sebagai berikut: import string teks = \"Prita adalah mahasiswa tahun ke-3 Program Studi S1 Teknik Informatika di Universitas Trunojoyo Madura. Karena masih situasi pandemi, Prita menempuh perkuliahan dari rumah secara daring.\" teks_baru = teks.translate(str.maketrans(\"\",\"\",string.punctuation)) print(teks_baru) Output akan seperti berikut ini: Prita adalah mahasiswa tahun ke3 Program Studi S1 Teknik Informatika di Universitas Trunojoyo Madura Karena masih situasi pandemi Prita menempuh perkuliahan dari rumah secara daring","title":"Menghilangkan tanda baca dalam teks"},{"location":"#tokenisasi","text":"","title":"Tokenisasi"},{"location":"#teori-tentang-tokenisasi","text":"Tokenizing atau tokenisasi dapat diartikan sebagai proses pemisahan kata-kata dalam suatu teks. Kata demi kata tersebut dipisahkan dan dikenal sebagai token. Dalam Python sendiri, ada syntax yang dapat digunakan untuk memisahkan kata dalam suatu kalimat/paragraf yaitu perintah split(). Namun biasanya dalam suatu paragraf terdapat kata yang berulang kali digunakan. Penggunaan perintah split() tidak lagi dapat membantu karena perintah tersebut hanya memisahkan kata tanpa mengenali apakah kata tersebut sudah ada sebelumnya atau belum. Untuk itu, ada modul NLTK milik Python yang memang ditujukan untuk Text Preprocesing. Perintah yang digunakan adalah word_tokenize()","title":"Teori tentang Tokenisasi"},{"location":"#implementasi-tokenisasi","text":"import nltk from nltk.tokenize import word_tokenize from nltk.probability import FreqDist import string teks = \"Prita adalah mahasiswa tahun ke-3 Program Studi Teknik Informatika di Universitas Trunojoyo Madura. Sebelum pandemi terjadi, biasanya Prita berangkat dari rumah ke kos menaiki bus. Setelah pandemi ada di Indonesia, Prita sama sekali belum kembali ke kos.\" teks = teks.translate(str.maketrans('','',string.punctuation)) kata = nltk.tokenize.word_tokenize(teks) frekuensi = nltk.FreqDist(kata) print(frekuensi.most_common()) Output akan seperti berikut ini: [('Prita', 3), ('di', 2), ('pandemi', 2), ('ke', 2), ('kos', 2), ('adalah', 1), ('mahasiswa', 1), ('tahun', 1), ('ke3', 1), ('Program', 1), ('Studi', 1), ('Teknik', 1), ('Informatika', 1), ('Universitas', 1), ('Trunojoyo', 1), ('Madura', 1), ('Sebelum', 1), ('terjadi', 1), ('biasanya', 1), ('berangkat', 1), ('dari', 1), ('rumah', 1), ('menaiki', 1), ('bus', 1), ('Setelah', 1), ('ada', 1), ('Indonesia', 1), ('sama', 1), ('sekali', 1), ('belum', 1), ('kembali', 1)]","title":"Implementasi Tokenisasi"},{"location":"#stemming","text":"","title":"Stemming"},{"location":"#teori-tentang-stemming","text":"Stemming adalah suatu proses untuk mengidentifikasi kata dasar dari suatu kata yang ada. Misalnya, ada kata \"melihat\" maka ketika dilakukan proses stemming, akan diperoleh kata dasar yaitu \"lihat\". Dalam Python, untuk melakukan stemming dapat menggunakan library NLTK, namun ada library khusus yang diciptakan untuk stemming kata dalam bahasa Indonesia yaitu Python Sastrawi.","title":"Teori tentang Stemming"},{"location":"#implementasi-stemming","text":"from Sastrawi.Stemmer.StemmerFactory import StemmerFactory factory = StemmerFactory() stemmer = factory.create_stemmer() teks = \"Prita selalu melakukan olahraga setiap hari selama satu jam. Menurut Prita, olahraga mampu membantunya memperbaiki pola tidur di malam hari.\" teks_baru = stemmer.stem(teks) print(teks_baru) Outputnya akan seperti ini: prita selalu laku olahraga tiap hari lama satu jam turut prita olahraga mampu bantu baik pola tidur di malam hari","title":"Implementasi Stemming"},{"location":"#stopword-removal","text":"","title":"Stopword Removal"},{"location":"#teori-tentang-stopword","text":"Dalam sebuah teks yang panjang, seringkali terdapat kata yang tidak terlalu penting seperti kata hubung \"di\", \"ke\", \"yang\" dan lain sebagainya. Kata-kata tersebut sebaiknya tidak terlalu diperhatikan karena memang tidak penting seperti kata lain yang mungkin saja menjadi kunci dari suatu teks. Kata-kata yang tidak penting tersebut dikenal sebagai stopword.","title":"Teori tentang Stopword"},{"location":"#implementasi-stopword-removal","text":"from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from nltk.tokenize import word_tokenize factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover() teks = \"Dilaporkan pada Selasa, 8 Juni 2021 terjadi kecelakaan tunggal di Jalan Pattimura. Korban adalah siswa SMA bernama Doni yang baru pulang sekolah. Korban mengaku mengantuk sehingga menabrak pohon di tepi jalan. Kondisi korban saat ini baik-baik saja dan hanya mengalami luka ringan.\" teks_baru = stopword.remove(teks) print(teks_baru) Output akan seperti ini: Dilaporkan Selasa, 8 Juni 2021 terjadi kecelakaan tunggal Jalan Pattimura. Korban siswa SMA bernama Doni baru pulang sekolah. Korban mengaku mengantuk menabrak pohon tepi jalan. Kondisi korban ini baik-baik dan mengalami luka ringan.","title":"Implementasi Stopword Removal"},{"location":"#reduksi-dimensi","text":"","title":"Reduksi Dimensi"},{"location":"#teori","text":"Dalam pengolahan dokumen yang berbasis teks, tentu saja terdapat banyak kosakata. Ketika beragam kosakata tersebut berjumlah sangat banyak, maka diperlukan reduksi dimensi berupa seleksi fitur. Fitur perlu diseleksi untuk menghindari kemunculan berulang suatu fitur dan hanya memilih fitur-fitur yang penting saja. Reduksi dimensi dengan melakukan seleksi fitur dapat diterapkan dengan menggunakan matriks TF-IDF (Term Frequency-Inverse Document Frequency)","title":"Teori"},{"location":"#implementasi","text":"Untuk membentuk matriks TF-IDF, dapat menggunakan library sklearn dari Python dan mengimport modul TfidfTransformer.","title":"Implementasi"},{"location":"#implementasi-text-preprocessing","text":"import numpy as np import PyPDF2 import doctest import sys from IPython.display import Image from requests_html import HTMLSession import matplotlib.pyplot as plt %matplotlib inline import networkx as nx from nltk.tokenize.punkt import PunktSentenceTokenizer from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer session = HTMLSession() r = session.get('https://suarabojonegoro.com/news/2021/03/15/kapolres-bojonegoro-beri-penghargaan-kepada-kades-kasun-dan-ketua-rukun-tetangga-telah-bantu-percepatan-tangani-covid-19') articles = r.html.find('div.entry-content') for item in articles: newsitem = item.find('div.entry-content-single', first=True) news = newsitem.text print(news) doc_tokenizer = PunktSentenceTokenizer() sentences_list = doc_tokenizer.tokenize(news) type(sentences_list) import string from Sastrawi.Stemmer.StemmerFactory import StemmerFactory# create stemmer factory = StemmerFactory() stemmer = factory.create_stemmer() import re dokumenre=[] for i in sentences_list: hasil = re.sub(r\"\\d+\", \"\", i) dokumenre.append(hasil) print(dokumenre) len(dokumenre) dokumen=[] for i in dokumenre: hasil = i.replace('\\n','') dokumen.append(hasil) print(dokumen) from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from nltk.tokenize import word_tokenize factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover() #a=cv_matrix.toarray() a=len(dokumen) dokumenstop=[] for i in range(0, a): sentence = stopword.remove(dokumen[i]) dokumenstop.append(sentence) print(dokumenstop) cv = CountVectorizer() cv_matrix = cv.fit_transform(dokumen) a=cv_matrix.toarray() a.shape from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from nltk.tokenize import word_tokenize factory = StopWordRemoverFactory() dokumenstop=[] for i in dokumen: output = i.translate(str.maketrans(\"\",\"\",string.punctuation)) dokumenstop.append(output) print(dokumenstop) factory = StemmerFactory() stemmer = factory.create_stemmer() dokumenstem=[] for i in dokumenstop: output = stemmer.stem(i) dokumenstem.append(output) print(dokumenstem) bag = cv.fit_transform(dokumenstem) print(cv.vocabulary_) print(cv.get_feature_names()) len(cv.get_feature_names()) bag = cv.fit_transform(dokumenstem) matrik_vsm=bag.toarray() matrik_vsm.shape matrik_vsm[0] import pandas as pd a=cv.get_feature_names() print(len(matrik_vsm[:,1])) print(len(matrik_vsm[:,1])) dfb =pd.DataFrame(data=matrik_vsm,index=list(range(1, len(matrik_vsm[:,1])+1, )),columns=[a]) dfb from sklearn.feature_extraction.text import TfidfTransformer tfidf = TfidfTransformer(use_idf=True,norm='l2',smooth_idf=True) tf=tfidf.fit_transform(cv.fit_transform(dokumenstem)).toarray() dfb =pd.DataFrame(data=tf,index=list(range(1, len(tf[:,1])+1, )),columns=[a]) dfb","title":"Implementasi Text Preprocessing"},{"location":"#modelling","text":"","title":"Modelling"},{"location":"#teori_1","text":"Text classification merupakan salah satu hal dalam pengolahan bahasa alami. Biasanya, text classification digunakan dalam analisis sentimen (bisa diterapkan untuk analisis sentimen media sosial), pelabelan topik, dan lain-lain.","title":"Teori"},{"location":"#implementasi_1","text":"Implementasi text classification melibatkan text preprocessing, maka saya akan memberikan contoh code untuk meringkas dokumen menggunakan metode LSA (Latent Semantic Analysis). Dokumen yang saya gunakan adalah berita dari media online yang linknya tertera pada code berikut: import codecs import string import operator import numpy as np from nltk.tokenize import sent_tokenize from nltk.corpus import stopwords from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.utils.extmath import randomized_svd from requests_html import HTMLSession from Sastrawi.Stemmer.StemmerFactory import StemmerFactory# create stemmer from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory #untuk stopword remove from nltk.tokenize.punkt import PunktSentenceTokenizer session = HTMLSession() r = session.get('https://suarabojonegoro.com/news/2021/02/27/10-orang-terjaring-razia-protokol-kesehatan-covid-19-di-bojonegoro') articles = r.html.find('div.entry-content') for item in articles: newsitem = item.find('div.entry-content-single', first=True) news = newsitem.text print(news) sent_tokenize_list = sent_tokenize(news) stopwords= set(StopWordRemoverFactory().get_stop_words()) #Menghitung TF-IDF vectorizer = TfidfVectorizer(stop_words=stopwords, use_idf=True, ngram_range = (1, 3)) X = vectorizer.fit_transform(sent_tokenize_list) X_T = np.transpose(X) U, Sigma, VT = randomized_svd(X_T, n_components=100, n_iter=100, random_state=None) k = 4 temp_k = k i = 0 index = 0 output = [] index_list = [] if temp_k <= 4: dic = {} for j in range(0, len(VT[0])): dic[j] = VT[0][j] dic_sort = sorted(dic.items(), key=operator.itemgetter(1)) index1 = dic_sort[-1][0] index2 = dic_sort[-2][0] index3 = dic_sort[-3][0] index4 = dic_sort[-4][0] list = [index1, index2,index3,index4] list = sorted(list) if k == 1: output.append(sent_tokenize_list[list[0]]) if k == 2: output.append(sent_tokenize_list[list[0]]) output.append(sent_tokenize_list[list[1]]) if k ==3: output.append(sent_tokenize_list[list[0]]) output.append(sent_tokenize_list[list[1]]) output.append(sent_tokenize_list[list[2]]) if k ==4: output.append(sent_tokenize_list[list[0]]) output.append(sent_tokenize_list[list[1]]) output.append(sent_tokenize_list[list[2]]) output.append(sent_tokenize_list[list[3]]) summarized_text = \" \".join(output) print(\"Hasil rangkuman: \\n\",summarized_text) Output program adalah hasil ringkasan dari berita sebagai berikut: Hasil rangkuman: Reporter: Yudianto SuaraBojonegoro.com \u2013 Sebanyak 10 orang warga terjaring operasi Yustisi Protokol Kesehatan Covid 19 yang dilakukan oleh Petugas Gabungan, Dari Polres Bojonegoro, Kodim Bojonegoro, dan Satpol PP Pemkab Bojonegoro, Sabtu (27/2/2021). (adsbygoogle = window.adsbygoogle || []).push({}); \u201cMereka pelanggar Covid 19 ini disangsi karena diketahui tidak mengunakan masker,\u201d Kata Benny. Dalam Kegiatan yang dilaksanakan Monitor Bangsit Pendisiplinan Prokes Covid 19 juga Melakukan Himbauan kepada Masyarakat agar mematuhi Protokol Kesehatan Covid-19. (Yud/Red) (adsbygoogle = window.adsbygoogle || []).push({});","title":"Implementasi"},{"location":"#evaluasi","text":"","title":"Evaluasi"},{"location":"#teori_2","text":"Evaluasi ringkasan otomatis dapat dilakukan dengan banyak metode. Yang paling sering digunakan adalah ROUGE-n. ROUGE sendiri merupakan seperangkat metrik untuk mengevaluasi ringkasan teks otomatis yang bekerja dengan cara membandingkan suatu ringkasan otomatis atau terjemahan dengan seperangkat rangkuman referensi. Pengukuran ROUGE terbagi menjadi beberapa: ROUGE-N yang digunakan untuk mengukur unigram, bigram, trigram, dan n-gram yang lebih tinggi lagi ROUGE-L yang digunakan untuk mengukur pencocokan kata terpanjang menggunakan LCS. ROUGE-S yang juga bisa disebut sebagai skip-gram cooccurence. Misalnya, skip-bigram mengukur overlap dari pasangan kata yang memiliki jumlah gaps maksimum sebanyak dua pada setiap kata. ROUGE-SU merupakan pengembangan dari ROUGE-S yang memperhatikan unigram. ROUGE-W yang menggunakan LCS namun tidak memberikan preferensi pada kalimat yang emiliki kata-kata yang lebih berurutan. Untuk mengukur keakuratan ringkasan harus menghitung Precision, Recall, dan F-Measure. Rumus menghitung recall adalah banyaknya kata yang overlap dibagi dengan banyaknya kata pada ringkasan rujukan. Rumus menghitung Precision adalah banyaknya kata yang overlap dibagi dengan banyaknya kata pada ringkasan mesin Rumus menghitung F-Measure adalah","title":"Teori"},{"location":"#implementasi_2","text":"from rouge.rouge import rouge_n_sentence_level summary_sentence = 'the capital of China is Beijing'.split() reference_sentence = 'Beijing is the Capital of China'.split() #Menghitung nilai rouge recall, precision, rouge = rouge_n_sentence_level(summary_senteence, reference_sentence, 2) print ('ROUGE-2-R', recall) print ('ROUGE-2-P', precision) print ('ROUGE-2-F', rouge) Output yang akan dihasilkan adalah berikut: ROUGE-2-R 0.6 ROUGE-2-P 0.6 ROUGE-2-F 0.6 ROUGE-2-R 0.6","title":"Implementasi"}]}